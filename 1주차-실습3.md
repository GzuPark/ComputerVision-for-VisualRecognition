
## AutoStiching

> 본 실습은 1주차 과제물입니다. 실습1,2를 마쳤을 경우 미리 실습하셔도 좋습니다.
- 본 문제의 가정: 해당 코드는 경로를 설정한 폴더에 있는 이미지 조각들을 자동으로 스티칭하고 있습니다.

# AutoStitching
이미지 조각들에 대해서 자동으로 파노라마 이미지를 제작

이미지 스티칭 자료 <br>
![default](https://user-images.githubusercontent.com/44772344/52432332-87109d80-2b4d-11e9-9820-33b882747a8c.PNG)

이미지 스티칭 과정 .gif  <br>
![ezgif com-video-to-gif](https://user-images.githubusercontent.com/44772344/52432205-38fb9a00-2b4d-11e9-935b-47eeedaf450d.gif)

이미지 스티칭 결과 <br>
![result](https://user-images.githubusercontent.com/44772344/52432277-66484800-2b4d-11e9-83e7-3148248986c1.jpg)

## 전체코드
[1주차-실습3.cpp](1주차-실습/Example_003.cpp)
## 부분 설명
파노라마를 만드는 코드에 대해서는 [실습2](1주차-실습/Example_002.cpp)에서 다뤘으므로 그것을 보시길 바랍니다.<br>
Main 함수부터 자세히 보면
```
	Image_feature Image_feature;


	String folderpath = "__이미지가 저장된 파일 경로";
	vector<String> filenames;
	glob(folderpath, filenames);

	cout << "\n------- file load ---------\n" << endl;

	for (size_t i = 0; i < filenames.size(); i++)
	{
		panorama.push_back(imread(filenames[i], IMREAD_COLOR));
		cout << filenames[i] << "  load" << endl;
	}
```
위 부분을 통해서 파노라마로 만들 사진들을 파일에서 다 불러옵니다.


```
typedef struct Image_feature 
{
	Mat descriptors;
	vector< KeyPoint > keypoint;
	Mat img;
}Image_feature;
```
->구조체 정의
```
Image_feature find_image_fature(Mat img)
{
	vector< KeyPoint > keypoint;
	Image_feature img_class;
	Mat img_g;
	Mat descriptors;

	cvtColor(img, img_g, COLOR_BGR2GRAY);

	SiftDescriptorExtractor detector;
	detector.detect(img_g, keypoint);
	SiftDescriptorExtractor extractor;
	extractor.compute(img_g, keypoint, descriptors);

	img_class.descriptors = descriptors;
	img_class.keypoint = keypoint;
	img_class.img = img;

	return img_class;
}
```
->SIFT를 이용해서 사진의 descriptor,keypoint,사진원본을 구조체 속에 저장해서 반환
```
   for (int i = 0; i < filenames.size() ; i++)
	{
		Image_feature = find_image_fature(panorama[i]);
		Image_array.push_back(Image_feature);
		cout <<  filenames[i] << " finish "<< endl;
	}
```
불러온 사진들을 함수 find_image_fature을 통해 <br>
특징점들을  뽑아서 맨위에서 정의 해둔 구조체 Image_feature안에 저장합니다.

다음 autostiching을 하기 위해서 영상 중에서 다른 영상들과 가장 많은 매칭 가능한 사진을 찾습니다.
```
        vector<int> image_match_count;
	int match_count = 0;
	int bef_match_count = 0;
	int max_match = 0;
	int max_count = 0;
	for (int i = 0; i < filenames.size(); i++)
	{
		for (int j = 0; j < filenames.size(); j++)
		{
			if (i != j)
			{
				if ((find_matches_percent(Image_array[i], Image_array[j])) >= 10)	match_count++;
			}
		}
		if (max_count < match_count) {
			max_match = i;
			max_count = match_count;
		}
		cout << filenames[i] << " be matching " << match_count << " images" << endl;

		match_count = 0;

	}
	cout << "---" << max_match << endl;
```
find_matches_percent를 통해서 두 사진의 관계도 percent를 구합니다<br>
find_matches_percent에 대해서는 밑에서 다뤘습니다

```
   Image_array.erase(Image_array.begin() + max_match);
```

이미지의 featcher가 저장되있는 vector --Image_array변수에서 위에서 찾은 max_match를 없애줍니다.

```
Mat Panorama = imread(filenames[max_match], IMREAD_COLOR);
```
max_match인 영상을 기준점으로 초기화 시킵니다.

```
        double match = 0;
	int maxj = 0;
	double maxper = 0;
	for (int i = 0; i < filenames.size()-1; i++)
	{
		
		Image_feature = find_image_fature(Panorama);

		for (int j = 0; j < Image_array.size(); j++)
		{
		
			match = find_matches_percent(Image_feature, Image_array[j]);

			if (match>maxper)
			{
				maxper = match;
				maxj = j;
			}
		}
		cout << "--maxmatchdone--" <<maxj <<endl;
		Panorama = panorama_stiching(Image_feature, Image_array[maxj]);
		cout << maxj << endl;
		Image_array.erase(Image_array.begin() + maxj);
		cv::namedWindow("test", CV_WINDOW_FREERATIO);
		cv::imshow("test", Panorama);
		cv::waitKey(20);
		maxper = 0;
	}
```
기준점과 가장 매칭도가 높은 영상을 찾아서 붙히고<br>
붙혀진 영상을 다시 기준점으로 잡습니다.
```
   Image_feature = find_image_fature(Panorama);
```
기준점의 feature을 찾아서 저장
```
                for (int j = 0; j < Image_array.size(); j++)
		{
		
			match = find_matches_percent(Image_feature, Image_array[j]);

			if (match>maxper)
			{
				maxper = match;
				maxj = j;
			}
		}
```
기준점과 가장 매칭도가 높은 영상을 찾습니다.
```
Panorama = panorama_stiching(Image_feature, Image_array[maxj]);
```
기준점을 stiching한 결과로 초기화

```
   double find_matches_percent(Image_feature img1, Image_feature img2)
```
img1과img2의 feature을 인자로 받아서 두 영상의 매칭 percent를 결과 값으로 return 해줍니다.

```
    Mat HomoMatrix = findHomography(img2_pt, img1_pt, RANSAC, 3, mask);
```
이 코드까지는 실습 2에서 설명했으므로 생략 합니다.
다른점  findHomography함수에서 마지막에 인자로 mask를 넣어 주면 RANSAC 기법의 인라이너는 1 아웃라이너는 0으로 매칭점에 대한 인라이너 아웃라이너를 나타냅니다. 이를 이용해서 전체 매칭점(인라이너+아웃라이너) 중 인라이너의 비율을 구해서 두 사진의 매칭 가능성(확률)을 구하게 됩니다. 
```
    Mat panorama_stiching(Image_feature img1, Image_feature img2)
```
사진 두개의 feature를 받아서 두개를 stiching 한 영상을 결과롤 return 해줍니다.<br>

```
        Mat matResult;
	Mat matPanorama;

	// 4개의 코너 구하기
	vector<Point2f> conerPt, conerPt_1;

	conerPt.push_back(Point2f(0, 0));
	conerPt.push_back(Point2f(img2.img.size().width, 0));
	conerPt.push_back(Point2f(0, img2.img.size().height));
	conerPt.push_back(Point2f(img2.img.size().width, img2.img.size().height));

	Mat P_Trans_conerPt;
	perspectiveTransform(Mat(conerPt), P_Trans_conerPt, HomoMatrix);

	// 이미지의 모서리 계산
	double min_x, min_y, max_x, max_y, bef_max_x;
	float min_x1, min_x2, min_y1, min_y2, max_x1, max_x2, max_y1, max_y2;

	min_x1 = min(P_Trans_conerPt.at<Point2f>(0).x, P_Trans_conerPt.at<Point2f>(1).x);
	min_x2 = min(P_Trans_conerPt.at<Point2f>(2).x, P_Trans_conerPt.at<Point2f>(3).x);
	min_y1 = min(P_Trans_conerPt.at<Point2f>(0).y, P_Trans_conerPt.at<Point2f>(1).y);
	min_y2 = min(P_Trans_conerPt.at<Point2f>(2).y, P_Trans_conerPt.at<Point2f>(3).y);
	max_x1 = max(P_Trans_conerPt.at<Point2f>(0).x, P_Trans_conerPt.at<Point2f>(1).x);
	max_x2 = max(P_Trans_conerPt.at<Point2f>(2).x, P_Trans_conerPt.at<Point2f>(3).x);
	max_y1 = max(P_Trans_conerPt.at<Point2f>(0).y, P_Trans_conerPt.at<Point2f>(1).y);
	max_y2 = max(P_Trans_conerPt.at<Point2f>(2).y, P_Trans_conerPt.at<Point2f>(3).y);
	min_x = min(min_x1, min_x2);
	min_y = min(min_y1, min_y2);
	max_x = max(max_x1, max_x2);
	max_y = max(max_y1, max_y2);

	// Transformation matrix
	Mat Htr = Mat::eye(3, 3, CV_64F);
	if (min_x < 0)
	{
		max_x = img1.img.size().width - min_x;
		Htr.at<double>(0, 2) = -min_x;
	}
	else
	{
		if (max_x < img1.img.size().width) max_x = img1.img.size().width;
	}

	if (min_y < 0)
	{
		max_y = img1.img.size().height - min_y;
		Htr.at<double>(1, 2) = -min_y;
	}
	else
	{
		if (max_y < img1.img.size().height) max_y = img1.img.size().height;
	}

	// 파노라마 만들기
	matPanorama = Mat(Size(max_x, max_y), CV_32F);
	warpPerspective(img1.img, matPanorama, Htr, matPanorama.size(), INTER_CUBIC, BORDER_CONSTANT, 0);
	warpPerspective(img2.img, matPanorama, (Htr*HomoMatrix), matPanorama.size(), INTER_CUBIC, BORDER_TRANSPARENT, 0);
```
이 코드는 기준이 아닌 붙이고자 하는 이미지의 각 모서리를 구하고, 그 모서리에 해당하는 좌표만 perspectiveTransform 해서 이동했을때의 좌표를 구합니다. 이를 이용해 기준이되는 이미지와 이동했을때 이미지의 상관관계를 통해서 두개를 스티칭한 이미지가 갖는 최대의 크기를 계산합니다. 이를 통해 이미지 사이즈를 갖은 `matPanorama` 를 만들어 놓고 기준점이 되는 이미지와 붙일 이미지를 이동한 후 두개를 `warpPerspective`를 이용해 각 이미지(기준점이 되는 이미지와 새로 붙일 이미지)를 와핑합니다.
*이 부분에대한 추가설명은 해당 페이지를 통해서 좀 더 자세히 설명하겠습니다.
```
        vector<Point2f> conerPt, conerPt_1;

	conerPt.push_back(Point2f(0, 0));
	conerPt.push_back(Point2f(img2.img.size().width, 0));
	conerPt.push_back(Point2f(0, img2.img.size().height));
	conerPt.push_back(Point2f(img2.img.size().width, img2.img.size().height));
        //img2사이즈로 conerPt에 초기화
	Mat P_Trans_conerPt;
	perspectiveTransform(Mat(conerPt), P_Trans_conerPt, HomoMatrix);
```
img2를 findHomography함수를 통해 구한 H matrix를 인자로 <br>perspectiveTransform(inputimage,outputimage,H-matrix); <br>
로 P_Trans_conerPt에 img2를 변환한 img를 넣습니다.
```
        double min_x, min_y, max_x, max_y, bef_max_x;
	float min_x1, min_x2, min_y1, min_y2, max_x1, max_x2, max_y1, max_y2;

	min_x1 = min(P_Trans_conerPt.at<Point2f>(0).x, P_Trans_conerPt.at<Point2f>(1).x);
	min_x2 = min(P_Trans_conerPt.at<Point2f>(2).x, P_Trans_conerPt.at<Point2f>(3).x);
	min_y1 = min(P_Trans_conerPt.at<Point2f>(0).y, P_Trans_conerPt.at<Point2f>(1).y);
	min_y2 = min(P_Trans_conerPt.at<Point2f>(2).y, P_Trans_conerPt.at<Point2f>(3).y);
	max_x1 = max(P_Trans_conerPt.at<Point2f>(0).x, P_Trans_conerPt.at<Point2f>(1).x);
	max_x2 = max(P_Trans_conerPt.at<Point2f>(2).x, P_Trans_conerPt.at<Point2f>(3).x);
	max_y1 = max(P_Trans_conerPt.at<Point2f>(0).y, P_Trans_conerPt.at<Point2f>(1).y);
	max_y2 = max(P_Trans_conerPt.at<Point2f>(2).y, P_Trans_conerPt.at<Point2f>(3).y);
	min_x = min(min_x1, min_x2);
	min_y = min(min_y1, min_y2);
	max_x = max(max_x1, max_x2);
	max_y = max(max_y1, max_y2);
```
 위 코드를 통해서 P_Trans_conerPt의 모서리의 위치값을 알아냅니다.
 min,max는 인자중에서 작고 큰것을 return 해주는 메소드 입니다
 ```
        Mat Htr = Mat::eye(3, 3, CV_64F);
	if (min_x < 0)
	{
		max_x = img1.img.size().width - min_x;
		Htr.at<double>(0, 2) = -min_x;
	}
	else
	{
		if (max_x < img1.img.size().width) max_x = img1.img.size().width;
	}

	if (min_y < 0)
	{
		max_y = img1.img.size().height - min_y;
		Htr.at<double>(1, 2) = -min_y;
	}
	else
	{
		if (max_y < img1.img.size().height) max_y = img1.img.size().height;
	}
 ```
 
